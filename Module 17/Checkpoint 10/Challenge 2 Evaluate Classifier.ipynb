{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: sentiment analysis with Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='  \\t', header=None, engine='python')\n",
    "#df = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "df.columns = ['Sentence', 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the bools for the score\n",
    "df.loc[df['Score'] == 0, 'Bool'] = True\n",
    "df.loc[df['Score'] == 1, 'Bool'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of words that are in the negative and positive comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = pd.DataFrame([])\n",
    "positive_words = pd.DataFrame([])\n",
    "lower_no_punc = pd.DataFrame([])\n",
    "#[^\\w\\d\\s]+\n",
    "for index in df.index:\n",
    "    if df.iloc[index]['Score'] == 0:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        negative_words = negative_words.append(out.lower().split())\n",
    "\n",
    "    if df.iloc[index]['Score'] == 1:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        positive_words = positive_words.append(out.lower().split())\n",
    "    lower_no_punc = lower_no_punc.append([out.lower().strip()])\n",
    "\n",
    "lower_no_punc = lower_no_punc.reset_index(drop=True)\n",
    "lower_no_punc.columns = ['Sentence']\n",
    "negative_words = pd.DataFrame(negative_words[0].unique())\n",
    "positive_words = pd.DataFrame(positive_words[0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping out the words that are in both positive and negative lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare word lists to each other and remove matching.\n",
    "neg_match = negative_words[0].isin(positive_words[0])\n",
    "pos_match = positive_words[0].isin(negative_words[0])\n",
    "\n",
    "for index in neg_match.index:\n",
    "    if neg_match.iloc[index] == True:\n",
    "        negative_words.drop(index, inplace=True)\n",
    "\n",
    "for index in pos_match.index:\n",
    "    if pos_match.iloc[index] == True:\n",
    "        positive_words.drop(index, inplace=True)\n",
    "        \n",
    "negative_words = negative_words.reset_index(drop=True)\n",
    "positive_words = positive_words.reset_index(drop=True)\n",
    "# Drops anything that is less then three words, reducing match errors.\n",
    "negative_words.drop(negative_words[negative_words[0].str.len() < 3].index, inplace=True)\n",
    "positive_words.drop(positive_words[positive_words[0].str.len() < 3].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputting the result of the word list to see if it matches the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neg in negative_words[0]:\n",
    "    df[str(neg)] = lower_no_punc['Sentence'].str.contains(str(neg), case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#Instantiate the model and store in a variable.\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[negative_words[0]]\n",
    "target = df['Bool']\n",
    "target = target.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 268\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bnb.fit(data, target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "I engineered the features to separate the words from the negative and positive comments and then subtracting out words that showed up in both lists.  This helped to create a good list the is representative of the negative comments.\n",
    "\n",
    "This method could be improved as around 27% of the comments not being accurately represented.  Leaving us with around a 70% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: evaluate your sentiment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using negative word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  False  True \n",
      "Actual                 \n",
      "False        500      0\n",
      "True         268    232\n",
      "With 20% Holdout: 0.595\n",
      "Testing on Sample: 0.732\n"
     ]
    }
   ],
   "source": [
    "con_matrix = pd.crosstab(target, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(con_matrix)\n",
    "\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all of the errors are when false should be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using positive words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 279\n",
      "Predicted  False  True \n",
      "Actual                 \n",
      "False        224    276\n",
      "True           3    497\n",
      "With 20% Holdout: 0.62\n",
      "Testing on Sample: 0.721\n"
     ]
    }
   ],
   "source": [
    "# Importing the data\n",
    "\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='  \\t', header=None, engine='python')\n",
    "#df = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "df.columns = ['Sentence', 'Score']\n",
    "#Setting the bools for the score\n",
    "df.loc[df['Score'] == 0, 'Bool'] = True\n",
    "df.loc[df['Score'] == 1, 'Bool'] = False\n",
    "\n",
    "negative_words = pd.DataFrame([])\n",
    "positive_words = pd.DataFrame([])\n",
    "lower_no_punc = pd.DataFrame([])\n",
    "\n",
    "for index in df.index:\n",
    "    if df.iloc[index]['Score'] == 0:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        negative_words = negative_words.append(out.lower().split())\n",
    "\n",
    "    if df.iloc[index]['Score'] == 1:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        positive_words = positive_words.append(out.lower().split())\n",
    "    lower_no_punc = lower_no_punc.append([out.lower().strip()])\n",
    "\n",
    "lower_no_punc = lower_no_punc.reset_index(drop=True)\n",
    "lower_no_punc.columns = ['Sentence']\n",
    "negative_words = pd.DataFrame(negative_words[0].unique())\n",
    "positive_words = pd.DataFrame(positive_words[0].unique())\n",
    "\n",
    "#Compare word lists to each other and remove matching.\n",
    "pos_match = positive_words[0].isin(negative_words[0])\n",
    "neg_match = negative_words[0].isin(positive_words[0])\n",
    "\n",
    "for index in pos_match.index:\n",
    "    if pos_match.iloc[index] == True:\n",
    "        positive_words.drop(index, inplace=True)\n",
    "        \n",
    "for index in neg_match.index:\n",
    "    if neg_match.iloc[index] == True:\n",
    "        negative_words.drop(index, inplace=True)\n",
    "\n",
    "positive_words = positive_words.reset_index(drop=True)\n",
    "negative_words = negative_words.reset_index(drop=True)\n",
    "# Drops anything that is less then three words, reducing match errors.\n",
    "positive_words.drop(positive_words[positive_words[0].str.len() < 3].index, inplace=True)\n",
    "negative_words.drop(negative_words[negative_words[0].str.len() < 3].index, inplace=True)\n",
    "\n",
    "for pos in positive_words[0]:\n",
    "    df[str(pos)] = lower_no_punc['Sentence'].str.contains(str(pos), case=False)\n",
    "    \n",
    "#Instantiate the model and store in a variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "data = df[positive_words[0]]\n",
    "target = df['Bool']\n",
    "target = target.astype('bool')\n",
    "bnb.fit(data, target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))\n",
    "con_matrix = pd.crosstab(target, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(con_matrix)\n",
    "\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have improved greatly on the positive word predictions begin more accurate, but with a drastic decrease of of our negative comments being accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word list non-comparison positive wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will just compile the word lists without comparing them to each other and eliminating them out if they are in the other word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 210\n",
      "Predicted  False  True \n",
      "Actual                 \n",
      "False        294    206\n",
      "True           4    496\n",
      "With 20% Holdout: 0.68\n",
      "Testing on Sample: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Importing the data\n",
    "\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='  \\t', header=None, engine='python')\n",
    "#df = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "df.columns = ['Sentence', 'Score']\n",
    "#Setting the bools for the score\n",
    "df.loc[df['Score'] == 0, 'Bool'] = True\n",
    "df.loc[df['Score'] == 1, 'Bool'] = False\n",
    "\n",
    "negative_words = pd.DataFrame([])\n",
    "positive_words = pd.DataFrame([])\n",
    "lower_no_punc = pd.DataFrame([])\n",
    "\n",
    "for index in df.index:\n",
    "    if df.iloc[index]['Score'] == 0:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        negative_words = negative_words.append(out.lower().split())\n",
    "\n",
    "    if df.iloc[index]['Score'] == 1:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        positive_words = positive_words.append(out.lower().split())\n",
    "    lower_no_punc = lower_no_punc.append([out.lower().strip()])\n",
    "\n",
    "lower_no_punc = lower_no_punc.reset_index(drop=True)\n",
    "lower_no_punc.columns = ['Sentence']\n",
    "negative_words = pd.DataFrame(negative_words[0].unique())\n",
    "positive_words = pd.DataFrame(positive_words[0].unique())\n",
    "\n",
    "#Compare word lists to each other and remove matching.\n",
    "#pos_match = positive_words[0].isin(negative_words[0])\n",
    "#neg_match = negative_words[0].isin(positive_words[0])\n",
    "\n",
    "# for index in pos_match.index:\n",
    "#     if pos_match.iloc[index] == True:\n",
    "#         positive_words.drop(index, inplace=True)\n",
    "        \n",
    "# for index in neg_match.index:\n",
    "#     if neg_match.iloc[index] == True:\n",
    "#         negative_words.drop(index, inplace=True)\n",
    "\n",
    "positive_words = positive_words.reset_index(drop=True)\n",
    "negative_words = negative_words.reset_index(drop=True)\n",
    "# Drops anything that is less then three words, reducing match errors.\n",
    "positive_words.drop(positive_words[positive_words[0].str.len() < 3].index, inplace=True)\n",
    "negative_words.drop(negative_words[negative_words[0].str.len() < 3].index, inplace=True)\n",
    "\n",
    "for pos in positive_words[0]:\n",
    "    df[str(pos)] = lower_no_punc['Sentence'].str.contains(str(pos), case=False)\n",
    "    \n",
    "#Instantiate the model and store in a variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "data = df[positive_words[0]]\n",
    "target = df['Bool']\n",
    "target = target.astype('bool')\n",
    "bnb.fit(data, target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))\n",
    "con_matrix = pd.crosstab(target, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(con_matrix)\n",
    "\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is good improvement on the negative comments being more accurately identified when just using the positive words list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word list non-comparison negative wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 133\n",
      "Predicted  False  True \n",
      "Actual                 \n",
      "False        489     11\n",
      "True         122    378\n",
      "With 20% Holdout: 0.705\n",
      "Testing on Sample: 0.867\n"
     ]
    }
   ],
   "source": [
    "# Importing the data\n",
    "\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='  \\t', header=None, engine='python')\n",
    "#df = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "df.columns = ['Sentence', 'Score']\n",
    "#Setting the bools for the score\n",
    "df.loc[df['Score'] == 0, 'Bool'] = True\n",
    "df.loc[df['Score'] == 1, 'Bool'] = False\n",
    "\n",
    "negative_words = pd.DataFrame([])\n",
    "positive_words = pd.DataFrame([])\n",
    "lower_no_punc = pd.DataFrame([])\n",
    "\n",
    "for index in df.index:\n",
    "    if df.iloc[index]['Score'] == 0:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        negative_words = negative_words.append(out.lower().split())\n",
    "\n",
    "    if df.iloc[index]['Score'] == 1:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        positive_words = positive_words.append(out.lower().split())\n",
    "    lower_no_punc = lower_no_punc.append([out.lower().strip()])\n",
    "\n",
    "lower_no_punc = lower_no_punc.reset_index(drop=True)\n",
    "lower_no_punc.columns = ['Sentence']\n",
    "negative_words = pd.DataFrame(negative_words[0].unique())\n",
    "positive_words = pd.DataFrame(positive_words[0].unique())\n",
    "\n",
    "#Compare word lists to each other and remove matching.\n",
    "#pos_match = positive_words[0].isin(negative_words[0])\n",
    "#neg_match = negative_words[0].isin(positive_words[0])\n",
    "\n",
    "# for index in pos_match.index:\n",
    "#     if pos_match.iloc[index] == True:\n",
    "#         positive_words.drop(index, inplace=True)\n",
    "        \n",
    "# for index in neg_match.index:\n",
    "#     if neg_match.iloc[index] == True:\n",
    "#         negative_words.drop(index, inplace=True)\n",
    "\n",
    "positive_words = positive_words.reset_index(drop=True)\n",
    "negative_words = negative_words.reset_index(drop=True)\n",
    "# Drops anything that is less then three words, reducing match errors.\n",
    "positive_words.drop(positive_words[positive_words[0].str.len() < 3].index, inplace=True)\n",
    "negative_words.drop(negative_words[negative_words[0].str.len() < 3].index, inplace=True)\n",
    "\n",
    "for neg in negative_words[0]:\n",
    "    df[str(neg)] = lower_no_punc['Sentence'].str.contains(str(neg), case=False)\n",
    "    \n",
    "#Instantiate the model and store in a variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "data = df[negative_words[0]]\n",
    "target = df['Bool']\n",
    "target = target.astype('bool')\n",
    "bnb.fit(data, target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))\n",
    "con_matrix = pd.crosstab(target, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(con_matrix)\n",
    "\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total amount of mislabels has decreased by 77 to our lowest yet when using the negative words list, wich has increased our accuracy.  It seems to have balanced out both the negative and false comments bringing the mislabels for each closer in count to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Negative no compare without deleting small words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 109\n",
      "Predicted  False  True \n",
      "Actual                 \n",
      "False        481     19\n",
      "True          90    410\n",
      "With 20% Holdout: 0.755\n",
      "Testing on Sample: 0.891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.65, 0.75, 0.68, 0.68, 0.72, 0.7 , 0.74, 0.65, 0.69, 0.73])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data\n",
    "\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='  \\t', header=None, engine='python')\n",
    "#df = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "df.columns = ['Sentence', 'Score']\n",
    "#Setting the bools for the score\n",
    "df.loc[df['Score'] == 0, 'Bool'] = True\n",
    "df.loc[df['Score'] == 1, 'Bool'] = False\n",
    "\n",
    "negative_words = pd.DataFrame([])\n",
    "positive_words = pd.DataFrame([])\n",
    "lower_no_punc = pd.DataFrame([])\n",
    "\n",
    "for index in df.index:\n",
    "    if df.iloc[index]['Score'] == 0:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        negative_words = negative_words.append(out.lower().split())\n",
    "\n",
    "    if df.iloc[index]['Score'] == 1:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        positive_words = positive_words.append(out.lower().split())\n",
    "    lower_no_punc = lower_no_punc.append([out.lower().strip()])\n",
    "\n",
    "lower_no_punc = lower_no_punc.reset_index(drop=True)\n",
    "lower_no_punc.columns = ['Sentence']\n",
    "negative_words = pd.DataFrame(negative_words[0].unique())\n",
    "positive_words = pd.DataFrame(positive_words[0].unique())\n",
    "\n",
    "#Compare word lists to each other and remove matching.\n",
    "#pos_match = positive_words[0].isin(negative_words[0])\n",
    "#neg_match = negative_words[0].isin(positive_words[0])\n",
    "\n",
    "# for index in pos_match.index:\n",
    "#     if pos_match.iloc[index] == True:\n",
    "#         positive_words.drop(index, inplace=True)\n",
    "        \n",
    "# for index in neg_match.index:\n",
    "#     if neg_match.iloc[index] == True:\n",
    "#         negative_words.drop(index, inplace=True)\n",
    "\n",
    "positive_words = positive_words.reset_index(drop=True)\n",
    "negative_words = negative_words.reset_index(drop=True)\n",
    "# Drops anything that is less then three words, reducing match errors.\n",
    "#positive_words.drop(positive_words[positive_words[0].str.len() < 3].index, inplace=True)\n",
    "#negative_words.drop(negative_words[negative_words[0].str.len() < 3].index, inplace=True)\n",
    "\n",
    "for neg in negative_words[0]:\n",
    "    df[str(neg)] = lower_no_punc['Sentence'].str.contains(str(neg), case=False)\n",
    "    \n",
    "#Instantiate the model and store in a variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "data = df[negative_words[0]]\n",
    "target = df['Bool']\n",
    "target = target.astype('bool')\n",
    "bnb.fit(data, target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))\n",
    "con_matrix = pd.crosstab(target, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(con_matrix)\n",
    "\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like decreasing the complexity and constraints on my initial classifier is increasing the accuracy of the model.  There is much improvment that can be done to the classifier as shown with the cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Positive no compare without deleting small words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 209\n",
      "Predicted  False  True \n",
      "Actual                 \n",
      "False        295    205\n",
      "True           4    496\n",
      "With 20% Holdout: 0.665\n",
      "Testing on Sample: 0.791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing the data\n",
    "\n",
    "df = pd.read_csv('imdb_labelled.txt', sep='  \\t', header=None, engine='python')\n",
    "#df = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "df.columns = ['Sentence', 'Score']\n",
    "#Setting the bools for the score\n",
    "df.loc[df['Score'] == 0, 'Bool'] = True\n",
    "df.loc[df['Score'] == 1, 'Bool'] = False\n",
    "\n",
    "negative_words = pd.DataFrame([])\n",
    "positive_words = pd.DataFrame([])\n",
    "lower_no_punc = pd.DataFrame([])\n",
    "\n",
    "for index in df.index:\n",
    "    if df.iloc[index]['Score'] == 0:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        negative_words = negative_words.append(out.lower().split())\n",
    "\n",
    "    if df.iloc[index]['Score'] == 1:\n",
    "        s = df.iloc[index]['Sentence']\n",
    "        out = re.sub(r'[{}]'.format(string.punctuation), ' ', s)\n",
    "        positive_words = positive_words.append(out.lower().split())\n",
    "    lower_no_punc = lower_no_punc.append([out.lower().strip()])\n",
    "\n",
    "lower_no_punc = lower_no_punc.reset_index(drop=True)\n",
    "lower_no_punc.columns = ['Sentence']\n",
    "negative_words = pd.DataFrame(negative_words[0].unique())\n",
    "positive_words = pd.DataFrame(positive_words[0].unique())\n",
    "\n",
    "#Compare word lists to each other and remove matching.\n",
    "#pos_match = positive_words[0].isin(negative_words[0])\n",
    "#neg_match = negative_words[0].isin(positive_words[0])\n",
    "\n",
    "# for index in pos_match.index:\n",
    "#     if pos_match.iloc[index] == True:\n",
    "#         positive_words.drop(index, inplace=True)\n",
    "        \n",
    "# for index in neg_match.index:\n",
    "#     if neg_match.iloc[index] == True:\n",
    "#         negative_words.drop(index, inplace=True)\n",
    "\n",
    "positive_words = positive_words.reset_index(drop=True)\n",
    "negative_words = negative_words.reset_index(drop=True)\n",
    "# Drops anything that is less then three words, reducing match errors.\n",
    "#positive_words.drop(positive_words[positive_words[0].str.len() < 3].index, inplace=True)\n",
    "#negative_words.drop(negative_words[negative_words[0].str.len() < 3].index, inplace=True)\n",
    "\n",
    "for pos in positive_words[0]:\n",
    "    df[str(pos)] = lower_no_punc['Sentence'].str.contains(str(pos), case=False)\n",
    "    \n",
    "#Instantiate the model and store in a variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "data = df[positive_words[0]]\n",
    "target = df['Bool']\n",
    "target = target.astype('bool')\n",
    "bnb.fit(data, target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))\n",
    "con_matrix = pd.crosstab(target, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(con_matrix)\n",
    "\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same does not seem true for the positive words list.  Reducing the complexity and constraints has improved this classifier, there is still some imbalance that is creating errors in this word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58208955, 0.65671642, 0.64179104, 0.59701493, 0.59701493,\n",
       "       0.64179104, 0.68656716, 0.67164179, 0.76119403, 0.65671642,\n",
       "       0.63636364, 0.65151515, 0.6969697 , 0.66666667, 0.71212121])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(bnb, data, target, cv=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cross validation confirms that this is true. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
